{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library:\n",
    "#!python setup.py install --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kdmkr import datasets\n",
    "from kdmkr import distillation\n",
    "from kdmkr import evaluation\n",
    "from kdmkr import loss\n",
    "from kdmkr import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creme import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device     = 'cuda'\n",
    "hidden_dim = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb15k237 = datasets.FB15K237(\n",
    "    batch_size=1024, \n",
    "    negative_sample_size=256, \n",
    "    shuffle=True, \n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = model.RotatE(\n",
    "    hidden_dim=1000, \n",
    "    n_entity=fb15k237.n_entity, \n",
    "    n_relation=fb15k237.n_relation, \n",
    "    gamma=9\n",
    ")\n",
    "teacher = teacher.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_teacher = torch.optim.Adam(filter(lambda p: p.requires_grad, teacher.parameters()), lr = 0.00005)\n",
    "\n",
    "max_step = 40000\n",
    "\n",
    "bar = tqdm.tqdm(range(1, max_step), position=0)\n",
    "\n",
    "metric = stats.RollingMean(1000)\n",
    "\n",
    "evaluation = evaluation.Evaluation()\n",
    "\n",
    "teacher.train()\n",
    "\n",
    "for step in bar:\n",
    "    \n",
    "    optimizer_teacher.zero_grad()\n",
    "    \n",
    "    positive_sample, negative_sample, weight, mode = next(fb15k237)\n",
    "    \n",
    "    positive_sample = positive_sample.to(device)\n",
    "    \n",
    "    negative_sample = negative_sample.to(device)\n",
    "    \n",
    "    weight = weight.to(device)\n",
    "    \n",
    "    positive_score = teacher(sample=positive_sample)\n",
    "    \n",
    "    negative_score = teacher(sample=(positive_sample, negative_sample), mode=mode)\n",
    "    \n",
    "    loss_teacher = loss.Adversarial()(positive_score, negative_score, weight, alpha=0.5)\n",
    "    \n",
    "    loss_teacher.backward()\n",
    "    \n",
    "    optimizer_teacher.step()\n",
    "    \n",
    "    metric.update(loss_teacher.item())\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        bar.set_description(f'Adversarial loss: {metric.get():6f}')\n",
    "    \n",
    "    if step % 2000 == 0:\n",
    "        \n",
    "        teacher = teacher.eval()\n",
    "        \n",
    "        score = evaluation(model=teacher, dataset=fb15k237.test_dataset(batch_size=8), device=device)\n",
    "        \n",
    "        teacher = teacher.train()\n",
    "        \n",
    "        print(score)\n",
    "        \n",
    "        # Set path HERE\n",
    "        with open(f'./models/teacher_fb15k237_{score}.pickle', 'wb') as handle:\n",
    "            \n",
    "            pickle.dump(teacher, handle, protocol = pickle.HIGHEST_PROTOCOL)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the previous training\n",
    "\n",
    "teacher_name = 'teacher_fb15k237_HITS@10: 0.519716, HITS@1: 0.233778, HITS@3: 0.364971, MR: 192.433475, MRR: 0.329106.pickle'\n",
    "\n",
    "with open(f'./models/{teacher_name}', 'rb') as handle:\n",
    "    \n",
    "    teacher = pickle.load(handle)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def init_tensor(batch_size_entity, head, relation, tail):\n",
    "    x = torch.zeros((1, batch_size_entity, 3))\n",
    "    x[:,:,0] = head\n",
    "    x[:,:,1] = relation\n",
    "    x[:,:,2] = tail\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40000 [00:00<?, ?it/s]/users/iris/rsourty/.local/lib/python3.6/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "Metric: 0.171015:   2%|▏         | 999/40000 [06:45<4:25:28,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.332967, HITS@1: 0.129850, HITS@3: 0.215846, MR: 447.010554, MRR: 0.198185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.042698:   5%|▍         | 1999/40000 [17:10<4:16:00,  2.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.362577, HITS@1: 0.139817, HITS@3: 0.231872, MR: 318.034447, MRR: 0.214035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.024577:   7%|▋         | 2999/40000 [27:37<4:01:18,  2.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.391625, HITS@1: 0.152057, HITS@3: 0.253029, MR: 272.911365, MRR: 0.231538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.016539:  10%|▉         | 3999/40000 [37:56<4:06:58,  2.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.413393, HITS@1: 0.157676, HITS@3: 0.266833, MR: 251.435014, MRR: 0.242154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.011931:  12%|█▏        | 4999/40000 [48:21<3:50:06,  2.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.436187, HITS@1: 0.174485, HITS@3: 0.286695, MR: 236.269789, MRR: 0.260792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.008846:  15%|█▍        | 5999/40000 [58:47<3:58:51,  2.37it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.444249, HITS@1: 0.178174, HITS@3: 0.294953, MR: 225.818211, MRR: 0.266622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.006723:  17%|█▋        | 6999/40000 [1:09:14<3:40:40,  2.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.459567, HITS@1: 0.191049, HITS@3: 0.310246, MR: 217.258575, MRR: 0.280450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.005277:  20%|█▉        | 7999/40000 [1:19:41<3:38:02,  2.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.471196, HITS@1: 0.198818, HITS@3: 0.320727, MR: 212.727744, MRR: 0.289335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.004231:  22%|██▏       | 8999/40000 [1:30:08<3:26:15,  2.51it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.477963, HITS@1: 0.201285, HITS@3: 0.326029, MR: 214.279390, MRR: 0.293161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.003501:  25%|██▍       | 9999/40000 [1:40:36<3:23:31,  2.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.486343, HITS@1: 0.207613, HITS@3: 0.333162, MR: 210.750733, MRR: 0.300148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.003016:  27%|██▋       | 10999/40000 [1:51:02<3:17:14,  2.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.490790, HITS@1: 0.210642, HITS@3: 0.337487, MR: 209.026972, MRR: 0.303541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.002590:  30%|██▉       | 11999/40000 [2:01:30<3:07:46,  2.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.491865, HITS@1: 0.211106, HITS@3: 0.337267, MR: 209.040311, MRR: 0.304476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.002337:  32%|███▏      | 12999/40000 [2:11:54<3:02:43,  2.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.495773, HITS@1: 0.215406, HITS@3: 0.342080, MR: 205.032688, MRR: 0.308445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.002109:  35%|███▍      | 13999/40000 [2:22:20<2:54:04,  2.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.504178, HITS@1: 0.216603, HITS@3: 0.346892, MR: 203.544098, MRR: 0.311779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.001961:  37%|███▋      | 14999/40000 [2:32:46<2:49:53,  2.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.504788, HITS@1: 0.219217, HITS@3: 0.349311, MR: 204.286671, MRR: 0.313773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.001830:  40%|███▉      | 15999/40000 [2:43:13<2:38:36,  2.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.503811, HITS@1: 0.218900, HITS@3: 0.348920, MR: 204.092299, MRR: 0.313747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.001746:  42%|████▏     | 16999/40000 [2:53:40<2:37:23,  2.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.505668, HITS@1: 0.220268, HITS@3: 0.349189, MR: 201.964258, MRR: 0.314533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.001658:  45%|████▍     | 17999/40000 [3:04:00<2:27:20,  2.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.506205, HITS@1: 0.222027, HITS@3: 0.353904, MR: 202.229234, MRR: 0.316922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.001574:  47%|████▋     | 18999/40000 [3:14:24<2:21:48,  2.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.506914, HITS@1: 0.219413, HITS@3: 0.350386, MR: 202.682571, MRR: 0.314565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.001551:  50%|████▉     | 19999/40000 [3:24:47<2:16:50,  2.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.505008, HITS@1: 0.218753, HITS@3: 0.348822, MR: 201.537257, MRR: 0.313770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.001535:  52%|█████▏    | 20999/40000 [3:35:12<2:04:23,  2.55it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.509186, HITS@1: 0.222051, HITS@3: 0.351876, MR: 202.310442, MRR: 0.317018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.001474:  55%|█████▍    | 21999/40000 [3:45:37<2:01:43,  2.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.506694, HITS@1: 0.218435, HITS@3: 0.350044, MR: 203.966261, MRR: 0.314046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.001452:  57%|█████▋    | 22999/40000 [3:56:01<1:54:52,  2.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.506425, HITS@1: 0.221025, HITS@3: 0.350191, MR: 205.250293, MRR: 0.315864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.001431:  60%|█████▉    | 23999/40000 [4:06:24<1:44:50,  2.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.508771, HITS@1: 0.222369, HITS@3: 0.353196, MR: 205.137667, MRR: 0.317222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.001401:  62%|██████▏   | 24999/40000 [4:16:50<1:42:13,  2.45it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.508013, HITS@1: 0.221514, HITS@3: 0.351143, MR: 205.272256, MRR: 0.316201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.001395:  65%|██████▍   | 25999/40000 [4:27:16<1:32:11,  2.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.510554, HITS@1: 0.222931, HITS@3: 0.354417, MR: 202.126869, MRR: 0.317738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.001375:  67%|██████▋   | 26999/40000 [4:37:40<1:28:51,  2.44it/s]  "
     ]
    }
   ],
   "source": [
    "# Number of entities to consider to distill:\n",
    "batch_size_entity = 20\n",
    "batch_size_relation = 20\n",
    "\n",
    "max_step = 40000\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "fb15k237 = datasets.FB15K237(\n",
    "    batch_size=1000, \n",
    "    negative_sample_size=1, \n",
    "    seed=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Increasing the size of latents representations of the student allow to improve results.\n",
    "student = model.RotatE(\n",
    "    hidden_dim=1000, \n",
    "    n_entity=fb15k237.n_entity, \n",
    "    n_relation=fb15k237.n_relation, \n",
    "    gamma=0\n",
    ")\n",
    "\n",
    "student = student.to(device)\n",
    "\n",
    "# Distillation process allow to handle different indexes between the student and the teacher.\n",
    "# Distillation process allow to pre-compute batch dedicated to distillation.\n",
    "distillation_process = distillation.Distillation(\n",
    "    teacher_entities  = fb15k237.entities, \n",
    "    student_entities  = fb15k237.entities, \n",
    "    teacher_relations = fb15k237.relations, \n",
    "    student_relations = fb15k237.relations,\n",
    ")\n",
    "\n",
    "optimizer_student = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, student.parameters()), lr = 0.00005)\n",
    "\n",
    "bar = tqdm.tqdm(range(1, max_step + 1), position=0)\n",
    "\n",
    "# Creme online metric.\n",
    "metric = stats.RollingMean(1000)\n",
    "\n",
    "teacher = teacher.eval()\n",
    "student = student.train()\n",
    "\n",
    "for step in bar:\n",
    "    \n",
    "    positive_sample, negative_sample, weight, mode = next(fb15k237)\n",
    "    \n",
    "    batch_tensor_head     = []\n",
    "    batch_tensor_relation = []\n",
    "    batch_tensor_tail     = []\n",
    "\n",
    "    entity_distribution = torch.tensor(\n",
    "        np.random.randint(low=0, high=fb15k237.n_entity, size=batch_size_entity)\n",
    "    ).view(1, batch_size_entity)\n",
    "    \n",
    "    relation_distribution = torch.tensor(\n",
    "        np.random.randint(low=0, high=fb15k237.n_relation, size=batch_size_relation)\n",
    "    ).view(1, batch_size_entity)\n",
    "\n",
    "    optimizer_student.zero_grad()\n",
    "    \n",
    "    for head, relation, tail in positive_sample:\n",
    "        \n",
    "        head, relation, tail = head.item(), relation.item(), tail.item()\n",
    "        \n",
    "        head_distribution = copy.deepcopy(entity_distribution)\n",
    "        head_distribution[0][0] = head\n",
    "        \n",
    "        tensor_head = init_tensor(\n",
    "            head     = head_distribution, \n",
    "            relation = relation, \n",
    "            tail     = tail, \n",
    "            batch_size_entity = batch_size_entity\n",
    "        )\n",
    "        \n",
    "        batch_tensor_head.append(tensor_head)\n",
    "        \n",
    "        tail_distribution = copy.deepcopy(entity_distribution)\n",
    "        tail_distribution[0][0] = tail\n",
    "        \n",
    "        tensor_tail = init_tensor(\n",
    "            head     = head, \n",
    "            relation = relation, \n",
    "            tail     = tail_distribution, \n",
    "            batch_size_entity = batch_size_entity\n",
    "        )\n",
    "        \n",
    "        batch_tensor_tail.append(tensor_tail)\n",
    "        \n",
    "        relation_pre_distribution = copy.deepcopy(relation_distribution)\n",
    "        relation_pre_distribution[0][0] = relation\n",
    "        \n",
    "        tensor_relation = init_tensor(\n",
    "            head     = head, \n",
    "            relation = relation_pre_distribution, \n",
    "            tail     = tail, \n",
    "            batch_size_entity = batch_size_relation,\n",
    "        )\n",
    "        \n",
    "        batch_tensor_relation.append(tensor_relation)\n",
    "        \n",
    "    # Create tensor [[(e_1, r_1, t_1),..(en, r_1, t_1)], ..,[(e1, r_3, t_2),..(e_n, r_3, t_2)]]\n",
    "    # Tensor of size (batch_size, number of entity needed to compute the distribution probability, 3)\n",
    "    teacher_head_tensor = torch.stack(batch_tensor_head).reshape(len(batch_tensor_head), batch_size_entity, 3).to(device=device, dtype=int)\n",
    "    student_head_tensor = torch.stack(batch_tensor_head).reshape(len(batch_tensor_head), batch_size_entity, 3).to(device=device, dtype=int)\n",
    "    \n",
    "    teacher_relation_tensor = torch.stack(batch_tensor_relation).reshape(len(batch_tensor_relation), batch_size_entity, 3).to(device=device, dtype=int)\n",
    "    student_relation_tensor = torch.stack(batch_tensor_relation).reshape(len(batch_tensor_relation), batch_size_entity, 3).to(device=device, dtype=int)\n",
    "    \n",
    "    teacher_tail_tensor = torch.stack(batch_tensor_tail).reshape(len(batch_tensor_tail), batch_size_entity, 3).to(device=device, dtype=int)\n",
    "    student_tail_tensor = torch.stack(batch_tensor_tail).reshape(len(batch_tensor_tail), batch_size_entity, 3).to(device=device, dtype=int)\n",
    "  \n",
    "    # Distillation loss of heads\n",
    "    loss_head = loss.KlDivergence()(\n",
    "        teacher_score=teacher.distill(teacher_head_tensor), \n",
    "        student_score=student.distill(student_head_tensor)\n",
    "    ) \n",
    "    \n",
    "    # Distillation loss of relations.\n",
    "    loss_relation = loss.KlDivergence()(\n",
    "        teacher_score=teacher.distill(teacher_relation_tensor), \n",
    "        student_score=student.distill(student_relation_tensor)\n",
    "    ) \n",
    "    \n",
    "    # Distillation loss of tails.\n",
    "    loss_tail = loss.KlDivergence()(\n",
    "        teacher_score=teacher.distill(teacher_tail_tensor), \n",
    "        student_score=student.distill(student_tail_tensor)\n",
    "    ) \n",
    "    \n",
    "    # The loss of the student is equal to the sum of all losses.\n",
    "    loss_student = loss_head + loss_relation + loss_tail\n",
    "    \n",
    "    metric.update(loss_student.item())\n",
    "    \n",
    "    loss_student.backward()\n",
    "\n",
    "    optimizer_student.step()\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "    \n",
    "        bar.set_description(f'Metric: {metric.get():6f}')\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        \n",
    "        student = student.eval()\n",
    "        \n",
    "        score = evaluation.Evaluation()(model=student, dataset=fb15k237.test_dataset(batch_size=8), device=device)\n",
    "        \n",
    "        print(score)\n",
    "        \n",
    "        with open(f'./models/student_fb15k237_{score}.pickle', 'wb') as handle:\n",
    "            \n",
    "            pickle.dump(student, handle, protocol = pickle.HIGHEST_PROTOCOL)    \n",
    "        \n",
    "        student = student.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python setup.py install --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kdmkr import datasets\n",
    "from kdmkr import distillation\n",
    "from kdmkr import evaluation\n",
    "from kdmkr import loss\n",
    "from kdmkr import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creme import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "hidden_dim = 5\n",
    "batch_size = 3 # 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn18rr = datasets.WN18RR(batch_size=batch_size, negative_sample_size=batch_size*2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = model.RotatE(hidden_dim=hidden_dim, n_entity=wn18rr.n_entity, n_relation=wn18rr.n_relation, gamma=6)\n",
    "teacher = teacher.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "distillation = distillation.Distillation(\n",
    "    teacher_entities  = wn18rr.entities, \n",
    "    student_entities  = wn18rr.entities, \n",
    "    teacher_relations = wn18rr.relations, \n",
    "    student_relations = wn18rr.relations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 2.576545: 100%|██████████| 1000/1000 [00:13<00:00, 75.96it/s]\n"
     ]
    }
   ],
   "source": [
    "optimizer_teacher = torch.optim.Adam(filter(lambda p: p.requires_grad, teacher.parameters()), lr = 0.0001)\n",
    "\n",
    "max_step = 1000\n",
    "\n",
    "bar = tqdm.tqdm(1, range(max_step), position=0)\n",
    "\n",
    "metric = stats.RollingMean(1000)\n",
    "evaluation = evaluation.Evaluation()\n",
    "\n",
    "teacher.train()\n",
    "\n",
    "for step in bar:\n",
    "    \n",
    "    optimizer_teacher.zero_grad()\n",
    "    \n",
    "    positive_sample, negative_sample, weight, mode = next(wn18rr)\n",
    "    \n",
    "    positive_sample = positive_sample.to(device)\n",
    "    negative_sample = negative_sample.to(device)\n",
    "    weight = weight.to(device)\n",
    "    \n",
    "    positive_score = teacher(sample=positive_sample)\n",
    "    \n",
    "    negative_score = teacher(sample=(positive_sample, negative_sample), mode=mode)\n",
    "    \n",
    "    loss_teacher = loss.Adversarial()(positive_score, negative_score, weight, alpha=1.5)\n",
    "    \n",
    "    loss_teacher.backward()\n",
    "    \n",
    "    optimizer_teacher.step()\n",
    "    \n",
    "    metric.update(loss_teacher.cpu())\n",
    "    \n",
    "    bar.set_description(f'Metric: {metric.get():6f}')\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        \n",
    "        evaluation(model=teacher, dataset=wn18rr.test_dataset(batch_size=8), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = model.RotatE(hidden_dim=hidden_dim, n_entity=wn18rr.n_entity, n_relation=wn18rr.n_relation, gamma=6)\n",
    "student = student.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/Users/home/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "Metric: 0.153018:  16%|█▌        | 162/1000 [03:49<19:26,  1.39s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-97924ae57535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m             loss_student += kl_divergence(\n\u001b[1;32m     75\u001b[0m                 \u001b[0mteacher_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_common_tail_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mstudent_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_common_tail_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             ) \n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/github/KD-MKR/kdmkr/model/rotate.py\u001b[0m in \u001b[0;36mdistill\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mre_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_score\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;34mr\"\"\"See :func:`torch.norm`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nuc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer_student = torch.optim.Adam(filter(lambda p: p.requires_grad, student.parameters()), lr = 0.0001)\n",
    "\n",
    "max_step = 1000\n",
    "\n",
    "bar = tqdm.tqdm(range(1, max_step), position=0)\n",
    "\n",
    "metric = stats.RollingMean(1000)\n",
    "\n",
    "teacher.eval()\n",
    "\n",
    "student.train()\n",
    "\n",
    "kl_divergence = loss.KlDivergence()\n",
    "\n",
    "for step in bar:\n",
    "    \n",
    "    positive_sample, _, _, _ = next(wn18rr)\n",
    "\n",
    "    for head, relation, tail in positive_sample:\n",
    "        \n",
    "        loss_student = 0\n",
    "        \n",
    "        optimizer_student.zero_grad()\n",
    "\n",
    "        head, relation, tail = head.item(), relation.item(), tail.item()\n",
    "\n",
    "        mode = distillation.distillation_mode(head=head, relation=relation, tail=tail)\n",
    "        \n",
    "        # TODO: Handle distinct entities\n",
    "        \n",
    "        if mode['head']:\n",
    "\n",
    "            teacher_common_head_sample, teacher_distinct_head_sample = distillation.mini_batch_teacher_head(\n",
    "                relation=relation, tail=tail)\n",
    "\n",
    "            student_common_head_sample, student_distinct_head_sample = distillation.mini_batch_student_head(\n",
    "                teacher_relation=relation, teacher_tail=tail)\n",
    "            \n",
    "            teacher_common_head_sample.to(device)\n",
    "            student_common_head_sample.to(device)\n",
    "            \n",
    "            loss_student += kl_divergence(\n",
    "                teacher_score=teacher.distill(teacher_common_head_sample), \n",
    "                student_score=student.distill(student_common_head_sample)\n",
    "            )\n",
    "\n",
    "        if mode['relation']:\n",
    "\n",
    "            teacher_common_relation_sample, teacher_distinct_relation_sample = distillation.mini_batch_teacher_relation(\n",
    "                head=head, tail=tail)\n",
    "\n",
    "            student_common_relation_sample, student_distinct_relation_sample = distillation.mini_batch_student_relation(\n",
    "                teacher_head=head, teacher_tail=tail)\n",
    "            \n",
    "            teacher_common_relation_sample.to(device)\n",
    "            student_common_relation_sample.to(device)\n",
    "\n",
    "            loss_student += kl_divergence(\n",
    "                teacher_score=teacher.distill(teacher_common_relation_sample), \n",
    "                student_score=student.distill(student_common_relation_sample)\n",
    "            )\n",
    "\n",
    "        if mode['tail']:\n",
    "\n",
    "            teacher_common_tail_sample, teacher_distinct_tail_sample = distillation.mini_batch_teacher_tail(\n",
    "                head=head, relation=relation)\n",
    "\n",
    "            student_common_tail_sample, student_distinct_tail_sample = distillation.mini_batch_student_tail(\n",
    "                teacher_head=head, teacher_relation=relation)\n",
    "            \n",
    "            teacher_common_tail_sample.to(device)\n",
    "            student_common_tail_sample.to(device)\n",
    "            \n",
    "            loss_student += kl_divergence(\n",
    "                teacher_score=teacher.distill(teacher_common_tail_sample), \n",
    "                student_score=student.distill(student_common_tail_sample)\n",
    "            ) \n",
    "        \n",
    "        loss_student.backward()\n",
    "\n",
    "        optimizer_student.step()\n",
    "    \n",
    "        metric.update(loss_student.cpu())\n",
    "    \n",
    "    bar.set_description(f'Metric: {metric.get():6f}')\n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        \n",
    "        evaluation(model=teacher, dataset=wn18rr.test_dataset(batch_size=8), device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

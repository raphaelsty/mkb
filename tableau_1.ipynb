{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python setup.py install --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall kdmkr -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kdmkr import datasets\n",
    "from kdmkr import distillation\n",
    "from kdmkr import evaluation\n",
    "from kdmkr import loss\n",
    "from kdmkr import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creme import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device     = 'cuda'\n",
    "hidden_dim = 500\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wn18rr = datasets.WN18RR(batch_size=batch_size, negative_sample_size=batch_size*2, shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "teacher = model.RotatE(hidden_dim=hidden_dim, n_entity=wn18rr.n_entity, n_relation=wn18rr.n_relation, gamma=6)\n",
    "teacher = teacher.to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimizer_teacher = torch.optim.Adam(filter(lambda p: p.requires_grad, teacher.parameters()), lr = 0.00005)\n",
    "\n",
    "max_step = 6000\n",
    "\n",
    "bar = tqdm.tqdm(range(1, max_step), position=0)\n",
    "\n",
    "metric = stats.RollingMean(1000)\n",
    "\n",
    "evaluation = evaluation.Evaluation()\n",
    "\n",
    "teacher.train()\n",
    "\n",
    "for step in bar:\n",
    "    \n",
    "    optimizer_teacher.zero_grad()\n",
    "    \n",
    "    positive_sample, negative_sample, weight, mode = next(wn18rr)\n",
    "    \n",
    "    positive_sample = positive_sample.to(device)\n",
    "    \n",
    "    negative_sample = negative_sample.to(device)\n",
    "    \n",
    "    weight = weight.to(device)\n",
    "    \n",
    "    positive_score = teacher(sample=positive_sample)\n",
    "    \n",
    "    negative_score = teacher(sample=(positive_sample, negative_sample), mode=mode)\n",
    "    \n",
    "    loss_teacher = loss.Adversarial()(positive_score, negative_score, weight, alpha=0.5)\n",
    "    \n",
    "    loss_teacher.backward()\n",
    "    \n",
    "    optimizer_teacher.step()\n",
    "    \n",
    "    metric.update(loss_teacher.item())\n",
    "    \n",
    "    if step % 30 == 0:\n",
    "    \n",
    "        bar.set_description(f'Metric: {metric.get():6f}')\n",
    "    \n",
    "    if step % 2000 == 0:\n",
    "        \n",
    "        teacher = teacher.eval()\n",
    "        \n",
    "        score = evaluation(model=teacher, dataset=wn18rr.test_dataset(batch_size=8), device=device)\n",
    "        \n",
    "        teacher = teacher.train()\n",
    "        \n",
    "        print(score)\n",
    "        \n",
    "        with open(f'/users/iris/rsourty/experiments/kdmkr/teacher_wn18rr_{score}.pickle', 'wb') as handle:\n",
    "            \n",
    "            pickle.dump(teacher, handle, protocol = pickle.HIGHEST_PROTOCOL)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_name = 'teacher_wn18rr_HITS@10: 0.527760, HITS@1: 0.419113, HITS@3: 0.477664, MR: 5509.747288, MRR: 0.457429.pickle'\n",
    "\n",
    "with open(f'/users/iris/rsourty/experiments/kdmkr/{teacher_name}', 'rb') as handle:\n",
    "    teacher = pickle.load(handle)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn18rr = datasets.WN18RR(batch_size=1, negative_sample_size=1, seed=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = model.RotatE(hidden_dim=hidden_dim, n_entity=wn18rr.n_entity, n_relation=wn18rr.n_relation, gamma=6)\n",
    "\n",
    "student = student.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "distillation_process = distillation.Distillation(\n",
    "    teacher_entities  = wn18rr.entities, \n",
    "    student_entities  = wn18rr.entities, \n",
    "    teacher_relations = wn18rr.relations, \n",
    "    student_relations = wn18rr.relations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]/users/iris/rsourty/.local/lib/python3.6/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "Metric: 0.162908:   2%|â–         | 1695/100000 [09:41<9:24:49,  2.90it/s]"
     ]
    }
   ],
   "source": [
    "student_batch_size = 2\n",
    "\n",
    "max_step = 100000\n",
    "\n",
    "wn18rr = datasets.WN18RR(batch_size=student_batch_size, negative_sample_size=1, seed=42, shuffle=True)\n",
    "\n",
    "student = model.RotatE(hidden_dim=500, n_entity=wn18rr.n_entity, n_relation=wn18rr.n_relation, gamma=0)\n",
    "\n",
    "student = student.to(device)\n",
    "\n",
    "optimizer_student = torch.optim.Adam(filter(lambda p: p.requires_grad, student.parameters()), lr = 0.0005)\n",
    "\n",
    "bar = tqdm.tqdm(range(1, max_step + 1), position=0)\n",
    "\n",
    "metric = stats.RollingMean(1000)\n",
    "\n",
    "teacher.eval()\n",
    "\n",
    "student.train()\n",
    "\n",
    "kl_divergence = loss.KlDivergence()\n",
    "\n",
    "for step in bar:\n",
    "    \n",
    "    positive_sample, _, _, _ = next(wn18rr)\n",
    "    \n",
    "    teacher_head = []\n",
    "    student_head = []\n",
    "    \n",
    "    teacher_relation = []\n",
    "    student_relation = []\n",
    "    \n",
    "    teacher_tail = []\n",
    "    student_tail = []\n",
    "    \n",
    "    optimizer_student.zero_grad()\n",
    "    \n",
    "    for head, relation, tail in positive_sample:\n",
    "        \n",
    "        head, relation, tail = head.item(), relation.item(), tail.item()\n",
    "\n",
    "        mode = distillation_process.distillation_mode(head=head, relation=relation, tail=tail)\n",
    "        \n",
    "        if mode['head']:\n",
    "            \n",
    "            teacher_common_head_sample, _ = distillation_process.mini_batch_teacher_head(\n",
    "                relation=relation, tail=tail)\n",
    "            \n",
    "            student_common_head_sample, _ = distillation_process.mini_batch_student_head(\n",
    "                teacher_relation=relation, teacher_tail=tail)\n",
    "            \n",
    "            teacher_head.append(teacher_common_head_sample)\n",
    "            student_head.append(student_common_head_sample)\n",
    "\n",
    "        if mode['relation']:\n",
    "            \n",
    "            teacher_common_relation_sample, _ = distillation_process.mini_batch_teacher_relation(\n",
    "                head=head, tail=tail)\n",
    "            \n",
    "            student_common_relation_sample, _ = distillation_process.mini_batch_student_relation(\n",
    "                teacher_head=head, teacher_tail=tail)\n",
    "            \n",
    "            teacher_relation.append(teacher_common_relation_sample)\n",
    "            student_relation.append(student_common_relation_sample)\n",
    "            \n",
    "        if mode['tail']:\n",
    "\n",
    "            teacher_common_tail_sample, _ = distillation_process.mini_batch_teacher_tail(\n",
    "                head=head, relation=relation)\n",
    "\n",
    "            student_common_tail_sample, _ = distillation_process.mini_batch_student_tail(\n",
    "                teacher_head=head, teacher_relation=relation)\n",
    "            \n",
    "            teacher_tail.append(teacher_common_tail_sample)\n",
    "            student_tail.append(student_common_tail_sample)\n",
    "            \n",
    "    teacher_head_tensor = torch.stack(teacher_head).reshape(len(teacher_head),  wn18rr.n_entity, 3).to(device)\n",
    "    student_head_tensor = torch.stack(student_head).reshape(len(student_head),  wn18rr.n_entity, 3).to(device)   \n",
    "      \n",
    "    teacher_relation_tensor = torch.stack(teacher_relation).reshape(len(teacher_relation), wn18rr.n_relation, 3).to(device)\n",
    "    student_relation_tensor = torch.stack(student_relation).reshape(len(student_relation), wn18rr.n_relation, 3).to(device)\n",
    "    \n",
    "    teacher_tail_tensor = torch.stack(teacher_tail).reshape(len(teacher_tail), wn18rr.n_entity, 3).to(device)\n",
    "    student_tail_tensor = torch.stack(student_tail).reshape(len(student_tail), wn18rr.n_entity, 3).to(device)\n",
    "    \n",
    "    loss_head = kl_divergence(\n",
    "        teacher_score=teacher.distill(teacher_head_tensor), \n",
    "        student_score=student.distill(student_head_tensor)\n",
    "    ) \n",
    "    \n",
    "    loss_relation = kl_divergence(\n",
    "        teacher_score=teacher.distill(teacher_relation_tensor), \n",
    "        student_score=student.distill(student_relation_tensor)\n",
    "    ) \n",
    "    \n",
    "    loss_tail = kl_divergence(\n",
    "        teacher_score=teacher.distill(teacher_tail_tensor), \n",
    "        student_score=student.distill(student_tail_tensor)\n",
    "    )\n",
    "    \n",
    "    loss_student = loss_head + loss_relation + loss_tail\n",
    "    \n",
    "    metric.update(loss_student.item())\n",
    "    \n",
    "    loss_student.backward()\n",
    "\n",
    "    optimizer_student.step()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "    \n",
    "        bar.set_description(f'Metric: {metric.get():6f}')\n",
    "    \n",
    "    if step % 5000 == 0:\n",
    "        \n",
    "        student = student.eval()\n",
    "        \n",
    "        score = evaluation.Evaluation()(model=student, dataset=wn18rr.test_dataset(batch_size=8), device=device)\n",
    "        \n",
    "        print(score)\n",
    "        \n",
    "        student = student.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

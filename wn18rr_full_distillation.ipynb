{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library:\n",
    "#!python setup.py install --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kdmkr import datasets\n",
    "from kdmkr import distillation\n",
    "from kdmkr import evaluation\n",
    "from kdmkr import loss\n",
    "from kdmkr import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from creme import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device     = 'cuda'\n",
    "hidden_dim = 500\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn18rr = datasets.WN18RR(\n",
    "    batch_size=512, \n",
    "    negative_sample_size=1024, \n",
    "    shuffle=True, \n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = model.RotatE(\n",
    "    hidden_dim=500, \n",
    "    n_entity=wn18rr.n_entity, \n",
    "    n_relation=wn18rr.n_relation, \n",
    "    gamma=6\n",
    ")\n",
    "teacher = teacher.to(\n",
    "    device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_teacher = torch.optim.Adam(filter(lambda p: p.requires_grad, teacher.parameters()), lr = 0.00005)\n",
    "\n",
    "max_step = 6000\n",
    "\n",
    "bar = tqdm.tqdm(range(1, max_step), position=0)\n",
    "\n",
    "metric = stats.RollingMean(1000)\n",
    "\n",
    "evaluation = evaluation.Evaluation()\n",
    "\n",
    "teacher.train()\n",
    "\n",
    "for step in bar:\n",
    "    \n",
    "    optimizer_teacher.zero_grad()\n",
    "    \n",
    "    positive_sample, negative_sample, weight, mode = next(wn18rr)\n",
    "    \n",
    "    positive_sample = positive_sample.to(device)\n",
    "    \n",
    "    negative_sample = negative_sample.to(device)\n",
    "    \n",
    "    weight = weight.to(device)\n",
    "    \n",
    "    positive_score = teacher(sample=positive_sample)\n",
    "    \n",
    "    negative_score = teacher(sample=(positive_sample, negative_sample), mode=mode)\n",
    "    \n",
    "    loss_teacher = loss.Adversarial()(positive_score, negative_score, weight, alpha=0.5)\n",
    "    \n",
    "    loss_teacher.backward()\n",
    "    \n",
    "    optimizer_teacher.step()\n",
    "    \n",
    "    metric.update(loss_teacher.item())\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        bar.set_description(f'Adversarial loss: {metric.get():6f}')\n",
    "    \n",
    "    if step % 2000 == 0:\n",
    "        \n",
    "        teacher = teacher.eval()\n",
    "        \n",
    "        score = evaluation(model=teacher, dataset=wn18rr.test_dataset(batch_size=8), device=device)\n",
    "        \n",
    "        teacher = teacher.train()\n",
    "        \n",
    "        print(score)\n",
    "        \n",
    "        # Set path HERE\n",
    "        with open(f'./models/teacher_wn18rr_{score}.pickle', 'wb') as handle:\n",
    "            \n",
    "            pickle.dump(teacher, handle, protocol = pickle.HIGHEST_PROTOCOL)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the previous training\n",
    "\n",
    "teacher_name = 'teacher_wn18rr_HITS@10: 0.527760, HITS@1: 0.419113, HITS@3: 0.477664, MR: 5509.747288, MRR: 0.457429.pickle'\n",
    "\n",
    "with open(f'./models/{teacher_name}', 'rb') as handle:\n",
    "    \n",
    "    teacher = pickle.load(handle)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def init_tensor(batch_size_entity, head, relation, tail):\n",
    "    x = torch.zeros((1, batch_size_entity, 3))\n",
    "    x[:,:,0] = head\n",
    "    x[:,:,1] = relation\n",
    "    x[:,:,2] = tail\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40000 [00:00<?, ?it/s]/users/iris/rsourty/.local/lib/python3.6/site-packages/torch/nn/functional.py:1958: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n",
      "Metric: 0.198259:   2%|▏         | 999/40000 [06:27<4:10:35,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.463944, HITS@1: 0.370932, HITS@3: 0.432355, MR: 6846.283982, MRR: 0.406958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.039777:   5%|▍         | 1999/40000 [14:17<4:01:37,  2.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.478622, HITS@1: 0.398213, HITS@3: 0.449426, MR: 6146.584397, MRR: 0.429363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.014932:   7%|▋         | 2999/40000 [22:05<3:49:00,  2.69it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.487556, HITS@1: 0.409860, HITS@3: 0.455648, MR: 5543.849234, MRR: 0.438895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.007452:  10%|▉         | 3999/40000 [29:35<3:36:52,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.493618, HITS@1: 0.412412, HITS@3: 0.460753, MR: 5318.804244, MRR: 0.442989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.004285:  12%|█▏        | 4999/40000 [37:11<3:28:44,  2.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HITS@10: 0.493140, HITS@1: 0.413210, HITS@3: 0.461391, MR: 5185.552489, MRR: 0.443610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric: 0.004266:  13%|█▎        | 5010/40000 [38:36<10:25:28,  1.07s/it] "
     ]
    }
   ],
   "source": [
    "student_batch_size = 1000\n",
    "\n",
    "# Number of entities to consider to distill:\n",
    "batch_size_entity = 20\n",
    "\n",
    "max_step = 40000\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "wn18rr = datasets.WN18RR(\n",
    "    batch_size=1000, \n",
    "    negative_sample_size=1, \n",
    "    seed=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Increasing the size of latents representations of the student allow to improve results.\n",
    "student = model.RotatE(\n",
    "    hidden_dim=1000, \n",
    "    n_entity=wn18rr.n_entity, \n",
    "    n_relation=wn18rr.n_relation, \n",
    "    gamma=0\n",
    ")\n",
    "\n",
    "student = student.to(device)\n",
    "\n",
    "# Distillation process allow to handle different indexes between the student and the teacher.\n",
    "# Distillation process allow to pre-compute batch dedicated to distillation.\n",
    "distillation_process = distillation.Distillation(\n",
    "    teacher_entities  = wn18rr.entities, \n",
    "    student_entities  = wn18rr.entities, \n",
    "    teacher_relations = wn18rr.relations, \n",
    "    student_relations = wn18rr.relations,\n",
    ")\n",
    "\n",
    "optimizer_student = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, student.parameters()), lr = 0.00005)\n",
    "\n",
    "bar = tqdm.tqdm(range(1, max_step + 1), position=0)\n",
    "\n",
    "# Creme online metric.\n",
    "metric = stats.RollingMean(1000)\n",
    "\n",
    "teacher = teacher.eval()\n",
    "student = student.train()\n",
    "\n",
    "for step in bar:\n",
    "    \n",
    "    positive_sample, negative_sample, weight, mode = next(wn18rr)\n",
    "    \n",
    "    batch_tensor_head = []\n",
    "    teacher_relation  = []\n",
    "    student_relation  = []\n",
    "    batch_tensor_tail = []\n",
    "\n",
    "    entity_distribution = torch.tensor(\n",
    "        np.random.randint(low=0, high=wn18rr.n_entity, size=batch_size_entity)\n",
    "    ).view(1, batch_size_entity)\n",
    "\n",
    "    optimizer_student.zero_grad()\n",
    "    \n",
    "    for head, relation, tail in positive_sample:\n",
    "        \n",
    "        head, relation, tail = head.item(), relation.item(), tail.item()\n",
    "        \n",
    "        head_distribution = copy.deepcopy(entity_distribution)\n",
    "        head_distribution[0][0] = head\n",
    "        \n",
    "        tensor_head = init_tensor(\n",
    "            head     = head_distribution, \n",
    "            relation = relation, \n",
    "            tail     = tail, \n",
    "            batch_size_entity = batch_size_entity\n",
    "        )\n",
    "        \n",
    "        tail_distribution = copy.deepcopy(entity_distribution)\n",
    "        tail_distribution[0][0] = tail\n",
    "        \n",
    "        tensor_tail = init_tensor(\n",
    "            head     = head, \n",
    "            relation = relation, \n",
    "            tail     = tail_distribution, \n",
    "            batch_size_entity = batch_size_entity\n",
    "        )\n",
    "        \n",
    "        batch_tensor_head.append(tensor_head)\n",
    "        batch_tensor_tail.append(tensor_tail)\n",
    "        \n",
    "        # Constructing the tensor dedicated to distillation of relation:\n",
    "        teacher_common_relation_sample, _ = distillation_process.mini_batch_teacher_relation(\n",
    "            head=head, tail=tail)\n",
    "\n",
    "        student_common_relation_sample, _ = distillation_process.mini_batch_student_relation(\n",
    "            teacher_head=head, teacher_tail=tail)\n",
    "\n",
    "        teacher_relation.append(teacher_common_relation_sample)\n",
    "        student_relation.append(student_common_relation_sample)\n",
    "        \n",
    "    # Create tensor [[(e_1, r_1, t_1),..(en, r_1, t_1)], ..,[(e1, r_3, t_2),..(e_n, r_3, t_2)]]\n",
    "    # Tensor of size (batch_size, number of entity needed to compute the distribution probability, 3)\n",
    "    teacher_head_tensor = torch.stack(batch_tensor_head).reshape(len(batch_tensor_head), batch_size_entity, 3).to(device=device, dtype=int)\n",
    "    student_head_tensor = torch.stack(batch_tensor_head).reshape(len(batch_tensor_head), batch_size_entity, 3).to(device=device, dtype=int)\n",
    "    \n",
    "    teacher_relation_tensor = torch.stack(teacher_relation).reshape(len(teacher_relation), wn18rr.n_relation, 3).to(device=device, dtype=int)\n",
    "    student_relation_tensor = torch.stack(student_relation).reshape(len(student_relation), wn18rr.n_relation, 3).to(device=device, dtype=int)\n",
    "    \n",
    "    teacher_tail_tensor = torch.stack(batch_tensor_tail).reshape(len(batch_tensor_tail), batch_size_entity, 3).to(device=device, dtype=int)\n",
    "    student_tail_tensor = torch.stack(batch_tensor_tail).reshape(len(batch_tensor_tail), batch_size_entity, 3).to(device=device, dtype=int)\n",
    "  \n",
    "    # Distillation loss of heads\n",
    "    loss_head = loss.KlDivergence()(\n",
    "        teacher_score=teacher.distill(teacher_head_tensor), \n",
    "        student_score=student.distill(student_head_tensor)\n",
    "    ) \n",
    "    \n",
    "    # Distillation loss of relations.\n",
    "    loss_relation = loss.KlDivergence()(\n",
    "        teacher_score=teacher.distill(teacher_relation_tensor), \n",
    "        student_score=student.distill(student_relation_tensor)\n",
    "    ) \n",
    "    \n",
    "    # Distillation loss of tails.\n",
    "    loss_tail = loss.KlDivergence()(\n",
    "        teacher_score=teacher.distill(teacher_tail_tensor), \n",
    "        student_score=student.distill(student_tail_tensor)\n",
    "    ) \n",
    "    \n",
    "    # The loss of the student is equal to the sum of all losses.\n",
    "    loss_student = loss_head + loss_relation + loss_tail\n",
    "    \n",
    "    metric.update(loss_student.item())\n",
    "    \n",
    "    loss_student.backward()\n",
    "\n",
    "    optimizer_student.step()\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "    \n",
    "        bar.set_description(f'Metric: {metric.get():6f}')\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        \n",
    "        student = student.eval()\n",
    "        \n",
    "        score = evaluation.Evaluation()(model=student, dataset=wn18rr.test_dataset(batch_size=8), device=device)\n",
    "        \n",
    "        print(score)\n",
    "        \n",
    "        with open(f'./models/student_wn18rr_{score}.pickle', 'wb') as handle:\n",
    "            \n",
    "            pickle.dump(student, handle, protocol = pickle.HIGHEST_PROTOCOL)    \n",
    "        \n",
    "        student = student.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
